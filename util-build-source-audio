#!/usr/bin/env python3

import fire
from classes import *
from pydub import AudioSegment
from classes.constants import *

MARGIN = 5


def cmd(transcript_path: str,  lang: str, timing_scheme: str, audio_output: str):
    transcript = Transcript.load(transcript_path)

    # dubbed = AudioSegment.silent(duration=transcript.phrases[-1].get_target(lang).end_time*1000+100)
    dubbed = AudioSegment.silent(duration=0)

    # dubbed.export(audio_output)

    # dubbed = AudioSegment.from_file(audio_output)

    behind = 0.0
    for phrase in transcript.phrases:
        gap = phrase.source.timings.get(timing_scheme).start_time - dubbed.duration_seconds

        if gap < 0:
            behind += -gap

        elif gap > 0.6 and behind > 0:
            if behind > 0.1:
                gap -= 0.1
                behind -= 0.1
            else:
                gap -= behind
                behind = 0.0

        if gap > 0.0:
            silence = int(gap * 1000)
            # print(f"Silence added: {silence}")
            dubbed = dubbed.append(AudioSegment.silent(duration=silence), crossfade=0)

        clip = AudioSegment.from_file(phrase.source.natural_audio_fullpath(service=SERVICE_SOURCE))
        dubbed = dubbed.append(clip, crossfade=0)
        print(f"Id: {phrase.id}  Start: {phrase.source.timings.get(timing_scheme).start_time}  Length: {clip.duration_seconds}")
        # dubbed = dubbed.overlay(
        #     clip,
        #     position=target.start_time * 1000,
        #     gain_during_overlay=30.0,
        # )
        # if phrase.id > 10:
        #     break

    tran_length = transcript.phrases[-1].get_timing(lang, timing_scheme).end_time
    if dubbed.duration_seconds < tran_length:
        dubbed = dubbed.append(AudioSegment.silent(duration=(tran_length-dubbed.duration_seconds)*1000), crossfade=0)

    dubbed.export(audio_output)
    print(f"Behind: {behind}")


if __name__ == "__main__":
    fire.Fire(cmd)
